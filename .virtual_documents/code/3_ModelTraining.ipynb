











# Import necessary libraries for data manipulation, machine learning, and visualization
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Markdown

# Set up visualization style
plt.style.use('fivethirtyeight')





# Load the dataset
try:
    data = pd.read_csv('./data/economic_indicators.csv', parse_dates=['DATE'])
    data.set_index('DATE', inplace=True)
    print(f"Successfully loaded data spanning from {data.index.min().date()} to {data.index.max().date()}")
except FileNotFoundError:
    print('Error: \'./data/economic_indicators.csv\' not found.')
    raise

# Display the first few rows of the dataset
display(data.head())


# Select key columns for analysis
key_columns = ['unemployment', 
               'yield_spread', 
               'SP500', 
               'CPI', 
               'recession_probabilities']

# Calculate daily changes for trend analysis
daily_changes = data[key_columns].diff().dropna()
daily_data = data[key_columns].loc[daily_changes.index]

# Combine raw data with change metrics
combined_data = pd.concat([daily_data, daily_changes.add_suffix('_change')], axis=1)

# Display the combined dataset
display(combined_data.head())





# Define a function to assign economic sentiment based on key indicators
def assign_sentiment(row):
    """
    Assigns economic sentiment based on S&P 500 changes and recession probability changes:
    - Positive: S&P 500 increasing with stable or decreasing recession probability
    - Negative: S&P 500 decreasing or significant increase in recession probability
    - Neutral: All other cases
    """
    if row['SP500_change'] > 0 and row['recession_probabilities_change'] <= 0:
        return 'positive'
    elif row['SP500_change'] < 0 or row['recession_probabilities_change'] > 5:
        return 'negative'
    else:
        return 'neutral'


# Apply sentiment classification to our dataset
combined_data['sentiment'] = combined_data.apply(assign_sentiment, axis=1)

# Visualize the distribution of sentiment values
sentiment_counts = combined_data['sentiment'].value_counts()
print("Sentiment Distribution:")
display(sentiment_counts)


# Prepare features and target for machine learning
features = [col for col in combined_data.columns if '_change' in col]
X = combined_data[features]
y = combined_data['sentiment']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.2, 
                                                    random_state=42)

# Initialize and train the Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, 
                                  random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate model performance
accuracy = rf_model.score(X_test, y_test)
print(f'Sentiment Classifier Accuracy: {accuracy:.2f}')

# Feature importance analysis
feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
display(feature_importance)





# Load GPT-2 model and tokenizer
try:
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model.eval()
    print("GPT-2 model successfully loaded")
except Exception as e:
    print(f'Error loading GPT-2: {e}')
    raise

# Set padding token
tokenizer.pad_token = tokenizer.eos_token


# Load FinBERT model and tokenizer
try:
    finbert_tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    finbert_model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    print("FinBERT model and tokenizer loaded successfully.")
except Exception as e:
    raise RuntimeError(f"Failed to load FinBERT model: {str(e)}")





def generate_narrative_gpt2(date, data, rf_model, finbert_model, finbert_tokenizer, model, tokenizer, max_length=150):
    """
    Generate an economic narrative for a specific date using GPT-2 with combined sentiment voting.
    
    Args:
        date: Target date for narrative
        data: DataFrame containing economic indicators
        rf_model: Trained Random Forest classifier
        finbert_model: FinBERT model for sentiment analysis
        finbert_tokenizer: FinBERT tokenizer
        model: GPT-2 language model
        tokenizer: GPT-2 tokenizer
        max_length: Maximum length of generated text
        
    Returns:
        String containing the economic narrative
    """
    if date not in data.index:
        nearest_idx = data.index.get_indexer([date], method='nearest')[0]
        nearest_date = data.index[nearest_idx]
        return (f'No data available for {date.strftime("%Y-%m-%d")}. '
                f'Nearest available date is {nearest_date.strftime("%Y-%m-%d")}.')
    
    row = data.loc[date]
    date_str = date.strftime('%B %d, %Y')
    
    # Rule-based sentiment
    def assign_sentiment(row):
        if row['SP500_change'] > 0 and row['recession_probabilities_change'] <= 0:
            return 'positive'
        elif row['SP500_change'] < 0 or row['recession_probabilities_change'] > 5:
            return 'negative'
        else:
            return 'neutral'
    rule_sentiment = assign_sentiment(row)
    
    # Random Forest sentiment
    change_cols = [col for col in data.columns if '_change' in col]
    change_data = pd.DataFrame([row[change_cols].values], columns=change_cols)
    rf_sentiment = rf_model.predict(change_data)[0]
    
    # FinBERT sentiment
    summary = (f"On {date_str}, unemployment was {row['unemployment']:.1f}%, "
               f"S&P 500 at {row['SP500']:.2f} ({row['SP500_change']:+.2f} points), "
               f"recession probability at {row['recession_probabilities']:.1f}%, "
               f"yield spread at {row['yield_spread']:.2f}, CPI at {row['CPI']:.1f}.")
    inputs = finbert_tokenizer(summary, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = finbert_model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        sentiment_score = torch.argmax(probs, dim=-1).item()
    finbert_sentiment = ["positive", "neutral", "negative"][sentiment_score]
    
    # Combine sentiments with a voting system
    sentiments = [rule_sentiment, rf_sentiment, finbert_sentiment]
    positive_count = sentiments.count("positive")
    negative_count = sentiments.count("negative")
    neutral_count = sentiments.count("neutral")

    if positive_count > max(negative_count, neutral_count):
        final_sentiment = "positive"
    elif negative_count > max(positive_count, neutral_count):
        final_sentiment = "negative"
    else:
        final_sentiment = "neutral"  # Default to neutral for ties or mixed signals
    
    # Generate narrative based on final sentiment
    sp500_change = row['SP500_change']
    sp500 = row['SP500']
    unemployment = row['unemployment']
    recession_prob = row['recession_probabilities']
    yield_spread = row['yield_spread']
    if final_sentiment == "positive":
        narrative = (f"Economic data for {date_str} points to a positive outlook. The S&P 500 rose by "
                     f"{sp500_change:.2f} points to {sp500:.2f}, reflecting market strength. Unemployment held steady "
                     f"at {unemployment:.1f}%, indicating a robust labor market. With a recession probability of "
                     f"{recession_prob:.1f}% and a yield spread of {yield_spread:.2f}, conditions suggest continued economic stability.")
    elif final_sentiment == "negative":
        narrative = (f"Economic indicators for {date_str} signal caution. The S&P 500 moved by {sp500_change:.2f} "
                     f"points to {sp500:.2f}, with unemployment at {unemployment:.1f}%. A recession probability of {recession_prob:.1f}% "
                     f"and yield spread of {yield_spread:.2f} raise concerns about economic uncertainty.")
    else:
        narrative = (f"On {date_str}, economic conditions appear mixed. The S&P 500 shifted by {sp500_change:.2f} "
                     f"points to {sp500:.2f}, while unemployment remained at {unemployment:.1f}%. A recession probability of "
                     f"{recession_prob:.1f}% and yield spread of {yield_spread:.2f} suggest a wait-and-see approach.")
    
    # Format change values for readability
    sp500_change_str = f"{row['SP500_change']:+.2f}"
    unemp_change_str = f"{row['unemployment_change']:+.2f}"
    recession_change_str = f"{row['recession_probabilities_change']:+.2f}"
    
    # Create prompt for GPT-2
    prompt = (
        f'On {date_str}, economic conditions showed a {final_sentiment} sentiment based on key indicators: '
        f'unemployment stood at {row["unemployment"]:.1f}% (change: {unemp_change_str}%), '
        f'the yield spread was {row["yield_spread"]:.2f} (change: {row["yield_spread_change"]:.2f}), '
        f'the S&P 500 was at {row["SP500"]:.2f} (change: {sp500_change_str} points), '
        f'CPI stood at {row["CPI"]:.1f} (change: {row["CPI_change"]:.2f}), and '
        f'recession probability was {row["recession_probabilities"]:.1f}% '
        f'(change: {recession_change_str}%). '
        'Generate a concise, coherent narrative summarizing these economic conditions.'
    )
    
    # Tokenize prompt
    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)
    
    # Generate narrative with GPT-2
    try:
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_length=max_length + len(inputs['input_ids'][0]),
                num_return_sequences=1,
                do_sample=True,
                top_k=30,
                top_p=0.85,
                temperature=0.5,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=2
            )
        
        # Extract only the generated part (not the prompt)
        gpt_narrative = tokenizer.decode(outputs[0], skip_special_tokens=True)
        gpt_narrative = gpt_narrative[len(prompt):].strip()
        
        # Format the final narrative with clear indicator highlights
        formatted_narrative = f"""## Economic Narrative for {date_str}

**Overall Sentiment:** {final_sentiment.upper()}

**Key Indicators:**
- Unemployment: {row['unemployment']:.1f}% ({unemp_change_str}%)
- S&P 500: {row['SP500']:.2f} ({sp500_change_str} points)
- Recession Probability: {row['recession_probabilities']:.1f}% ({recession_change_str}%)
- Yield Spread: {row['yield_spread']:.2f} ({row['yield_spread_change']:.2f})
- CPI: {row['CPI']:.1f} ({row['CPI_change']:.2f})

**Narrative:**
{narrative}

**GPT-2 Enhanced Narrative:**
{gpt_narrative}"""
        
        return formatted_narrative
    except Exception as e:
        return f'Error generating narrative: {e}'





# Define sample dates covering different economic periods
sample_dates = [
    pd.to_datetime('1990-01-03'),  # Early 90s
    pd.to_datetime('2008-01-02'),  # Start of 2008 financial crisis
    pd.to_datetime('2019-04-05'),  # Pre-COVID economy
    pd.to_datetime('2020-04-17'),  # COVID economic impact
    pd.to_datetime('2025-04-08'),  # Tariff economic impact
]

# Generate and display narratives for each date
for date in sample_dates:
    narrative = generate_narrative_gpt2(date, combined_data, rf_model, finbert_model, finbert_tokenizer, model, tokenizer)
    display(Markdown(narrative))
    print("\n" + "-"*80 + "\n")





# Test with potentially problematic dates
edge_case_dates = [
    pd.to_datetime('1990-03-10'),  # Weekend date
    pd.to_datetime('2024-08-14'),  # Past date
    pd.to_datetime('2025-04-25')   # Future date
]

for date in edge_case_dates:
    narrative = generate_narrative_gpt2(date, combined_data, rf_model, finbert_model, finbert_tokenizer, model, tokenizer)
    display(Markdown(narrative))
    print('\n' + '-'*80 + '\n')





# 1. Time Series Plot of Key Indicators
plt.figure(figsize=(14, 7))

# Define a colorblind-friendly palette manually (avoid red and yellow)
colorblind_palette = ['#0072B2', '#009E73', '#56B4E9', '#CC79A7', '#5D3A9B']
line_styles = ['-', '--', '-.', ':', '-']

# Plot each column with a unique color and line style
for i, col in enumerate(key_columns):
    plt.plot(combined_data.index, 
             combined_data[col], 
             label=col, 
             color=colorblind_palette[i % len(colorblind_palette)], 
             linestyle=line_styles[i % len(line_styles)])

plt.title('Economic Indicators Over Time', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Value', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()

# Add recession highlight periods with a blue shade instead of red
recession_periods = combined_data[combined_data['recession_probabilities'] > 70].index
if len(recession_periods) > 0:
    plt.axvspan(recession_periods.min(), recession_periods.max(), color='#56B4E9', alpha=0.2, label='High Recession Probability')

plt.savefig('./images/indicators_over_time.png', dpi=300)
plt.show();


# 2. Sentiment Distribution with enhanced styling
plt.figure(figsize=(10, 6))

# Create colorblind-friendly palette for sentiment (avoiding red and yellow)
sentiment_palette = {'positive': '#009E73',  # Green-blue
                     'neutral': '#0072B2',   # Blue
                     'negative': '#CC79A7'}  # Purple-pink

# Fix the countplot to use hue parameter
ax = sns.countplot(x='sentiment', 
                  hue='sentiment',
                  data=combined_data, 
                  palette=sentiment_palette,
                  legend=False,
                  order=['positive', 'neutral', 'negative'])

# Add count labels on top of bars
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', 
                (p.get_x() + p.get_width()/2., p.get_height()), 
                ha='center', va='bottom', fontsize=12)

plt.title('Distribution of Economic Sentiment', fontsize=16)
plt.xlabel('Sentiment Category', fontsize=14)
plt.ylabel('Count of Days', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()

plt.savefig('./images/sentiment_distribution.png', dpi=300)
plt.show();


# 3. Correlation Heatmap with improved readability
# Encode sentiment numerically: positive=1, neutral=0, negative=-1
combined_data['sentiment_numeric'] = combined_data['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': -1})
numeric_features = features + ['sentiment_numeric']

# Create more readable correlation labels
label_map = {
    'unemployment_change': 'Unemp. Δ',
    'yield_spread_change': 'Yield Spread Δ',
    'SP500_change': 'S&P 500 Δ',
    'CPI_change': 'CPI Δ',
    'recession_probabilities_change': 'Recession Prob. Δ',
    'sentiment_numeric': 'Sentiment'
}

plt.figure(figsize=(12, 10))
correlation = combined_data[numeric_features].corr()

# Rename correlation columns/index
correlation = correlation.rename(columns=label_map, index=label_map)

# Create the heatmap with colorblind-friendly colormap
mask = np.triu(np.ones_like(correlation, dtype=bool))
sns.heatmap(correlation, 
            annot=True, 
            cmap='PuBuGn_r',  # Colorblind-friendly purple-blue-green map
            linewidths=0.5,
            mask=mask,
            center=0,
            fmt='.2f',
            cbar_kws={'label': 'Correlation Coefficient'})

plt.title('Correlation Between Economic Indicators and Sentiment', fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()

plt.savefig('./images/correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show();


# 4. Sentiment over time visualization
plt.figure(figsize=(16, 6))

# Convert sentiment to numeric for plotting
# Using a colorblind-friendly palette (avoiding red and yellow)
sentiment_colors = {
    'positive': '#009E73',  # Green-blue
    'neutral': '#0072B2',   # Blue
    'negative': '#CC79A7'   # Purple-pink
}
combined_data['sentiment_value'] = combined_data['sentiment_numeric']

# Create multi-color scatter plot
for sentiment in sentiment_colors:
    mask = combined_data['sentiment'] == sentiment
    plt.scatter(combined_data.index[mask], 
                combined_data['sentiment_value'][mask],
                color=sentiment_colors[sentiment],
                alpha=0.7,
                s=20,
                label=sentiment)

plt.title('Economic Sentiment Timeline', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Sentiment', fontsize=14)
plt.yticks([-1, 0, 1], ['Negative', 'Neutral', 'Positive'], fontsize=12)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()

plt.savefig('./images/sentiment_timeline.png', dpi=300)
plt.show();









